{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c1e5db-b21e-4588-b942-3c76634e63b1",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: solid;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\n",
    "  border-radius: 10px;\">\n",
    "  \n",
    "# **Python et intelligence artificielle**\n",
    "\n",
    "# *Séance n°11a : Perceptron multicouche fait maison (partie 1)*\n",
    "\n",
    "</div>\n",
    "\n",
    "Durant ces deux séances, nous allons continuer notre exploration de l'intelligence\n",
    "artificielle avec la construction d'un **réseau de neurones artificiel** que nous\n",
    "associerons à un problème d'apprentissage supervisé. Ce réseau de neurones multicouche\n",
    "(appelé aussi **perceptron multicouche**) sera implémenté sans utiliser de bibliothèques\n",
    "spécialisées telles que *Scikit-learn*, *Keras* ou *TensorFlow*. Ce type de réseau\n",
    "constitue la base des réseaux de neurones profonds et il nous permettra de résoudre des\n",
    "tâches de classification plus ou moins complexes.\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs\n",
    "\n",
    "Au cours de ces deux séances, vous allez :\n",
    "\n",
    "- implémenter pas à pas un perceptron multicouche sans utiliser de bibliothèques spécialisées (*Scikit-learn*, *Keras* ou *TensorFlow*) ;\n",
    "- appliquer ce modèle au problème de classification binaire déjà abordé dans la [séance n°9](seance_09.ipynb) ;\n",
    "- comparer la performance de ce perceptron fait maison avec celui implémenté dans *Scikit-learn*.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Les réseaux de neurones artificiels sont des modèles inspirés des réseaux neuronaux\n",
    "biologiques. Ils sont adaptés à l'apprentissage des relations complexes dans les données\n",
    "pour y reconnaître ou générer des motifs. Ces réseaux utilisent des unités de calcul\n",
    "appelées **neurones** et reliées par des **poids** ajustables. Ces neurones sont\n",
    "organisés en **couches** successives, chacune jouant un rôle différent dans le traitement\n",
    "des données.\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "Le modèle du **perceptron** proposé par [Rosenblatt](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf)\n",
    "en 1958 a été l'un des premiers exemples de neurone artificiel. Bien que le perceptron\n",
    "simple soit limité aux problèmes de classification linéaire, l'ajout de couches cachées\n",
    "et de fonctions d'activation non linéaires a donné naissance au **perceptron multicouche** (MLP),\n",
    "capable de traiter des problèmes plus complexes et non linéaires.\n",
    "\n",
    "### Structure et fonctionnement d'un perceptron multicouche\n",
    "\n",
    "Un perceptron multicouche se compose de trois types de couches :\n",
    "\n",
    "1. Une **couche d'entrée** dans laquelle chaque neurone représente une caractéristique des données d'entrée.\n",
    "2. Une ou plusieurs **couches cachées** qui permettent l'apprentissage de relations non linéaires entre les caractéristiques des données.\n",
    "3. Une **couche de sortie** chargée de la prédiction finale du réseau.\n",
    "\n",
    "Dans chaque neurone, les données d'entrée ($x_1, x_2, \\cdots, x_n$) sont pondérées\n",
    "par des **poids** ($w_1, w_2, \\cdots, w_n$), et un **biais** $b$ est ajouté à la\n",
    "somme des entrées pondérées. Ce résultat est ensuite transformé par une\n",
    "**fonction d'activation** $\\phi$ de manière à générer la sortie $y$ du neurone :\n",
    "\n",
    "$$\n",
    "y = \\phi(z)\n",
    "$$\n",
    "\n",
    "où $z$ est la somme pondérée définie par :\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "### Fonction d'activation\n",
    "\n",
    "La fonction d'activation $\\phi$ qui agit à la sortie pondérée d'un neurone\n",
    "permet d'introduire une non-linéarité dans le modèle. Cette non-linéarité est\n",
    "essentielle pour permettre au réseau l'apprentissage des relations complexes\n",
    "entre les données. Voici quelques fonctions d'activation couramment utilisées :\n",
    "\n",
    "| Fonction d'activation | Formule | Avantages |\n",
    "|-----------------------|---------|-----------|\n",
    "| **sigmoïde**          | $\\phi(z) = \\frac{1}{1 + e^{-z}}$ | Idéale pour la classification binaire, fournit une valeur entre 0 et 1 |\n",
    "| **tanh**              | $\\phi(z) = \\tanh(z)$ | Produit des sorties entre -1 et 1, avec une sensibilité élevée aux petites variations |\n",
    "| **ReLU**              | $\\phi(z) = \\max(0, z)$ | Rapide à calculer, elle facilite la convergence dans les réseaux profonds |\n",
    "| **seuil**             | $\\phi(z) = \\begin{cases} 1 & \\text{si } z \\geq 0 \\\\ 0 & \\text{sinon} \\end{cases}$ | Simple, mais limitée aux frontières linéaires |\n",
    "\n",
    "---\n",
    "\n",
    "## Travail demandé\n",
    "\n",
    "Nous allons construire progressivement notre réseau de neurones en réalisant les étapes suivantes :\n",
    "\n",
    "1. **Préparation des données** : importation, encodage et standardisation des données.\n",
    "2. **Implémentation d'un neurone** : construction d'une unité neuronale avec sa fonction d'activation.\n",
    "3. **Construction d'une couche de neurones** : création d'une structure en couches.\n",
    "4. **Assemblage du réseau de neurones** : création d'un perceptron multicouche.\n",
    "5. **Entraînement avec rétropropagation** : introduction de l'algorithme de rétropropagation pour ajuster les poids et optimiser le modèle.\n",
    "6. **Améliorations et ajustements** : mise en oeuvre de la régularisation L2.\n",
    "7. **Comparaison des performances** : évaluation et comparaison des résultats obtenus avec ceux du modèle proposé par *Scikit-learn*.\n",
    "\n",
    "### 1. Préparation des données\n",
    "\n",
    "Tous les systèmes d'apprentissage automatique nécessitent une préparation des données\n",
    "soignée afin de garantir leur efficacité, et les réseaux de neurones n'y font pas\n",
    "exception. Les étapes de cette préparation incluent généralement :\n",
    "\n",
    "1. L'**importation** des données.\n",
    "2. L'**encodage** de la cible en valeurs numériques si cela est nécessaire.\n",
    "3. La **division** des données en un ensemble d'entraînement et un ensemble de tests.\n",
    "4. La **standardisation** ou la **normalisation** des données pour que la distribution soit uniforme et facilite la convergence.\n",
    "\n",
    "Le code suivant réalise ces différentes opérations sur le jeu de données \"`sonar.csv`\" que nous avons déjà rencontré lors de la séance n°9.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importation des données avec Pandas\n",
    "data = pd.read_csv('ressources/sonar.csv')\n",
    "\n",
    "# Préparation des données\n",
    "X = data.iloc[:, :-1].values  # 60 caractéristiques d'entrée\n",
    "y = data.iloc[:, -1].apply(lambda x: 1 if x == 'M' else 0).values  # Encodage de la cible en valeurs binaires\n",
    "\n",
    "# Division des données en deux jeux : entraînement et test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "Les données sont maintenant prêtes pour l'apprentissage.\n",
    "\n",
    "### 2. Implémentation d'un neurone\n",
    "\n",
    "Un perceptron multicouche est composé de plusieurs neurones, chacun effectuant une\n",
    "opération de base : prendre plusieurs entrées, les pondérer, ajouter un biais, puis\n",
    "appliquer une fonction d'activation.\n",
    "\n",
    "#### Fonctionnement du neurone\n",
    "\n",
    "Chaque neurone reçoit un vecteur d'entrées $X = [x_1, x_2, ..., x_n]$ dont il\n",
    "calcule la somme pondérée, ajoute un biais, et enfin transmet le résultat à une\n",
    "fonction d'activation pour produire une sortie entre 0 et 1 :\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^{n} w_i x_i + b\n",
    "$$\n",
    "\n",
    "La fonction d'activation, par exemple la fonction sigmoïde, transforme cette somme\n",
    "pondérée en une probabilité :\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Nous rappelons que la fonction sigmoïde est utile pour les problèmes de classification\n",
    "binaire, car elle génère une sortie proche de 1 pour une classe et proche de 0 pour\n",
    "l'autre, facilitant ainsi la prise de décision. Toutefois, l'utilisation d'un seul\n",
    "neurone reste limitée. Celui-ci ne peut capturer qu'une frontière de décision linéaire\n",
    "et ne peut pas résoudre des tâches nécessitant une complexité plus élevée.\n",
    "\n",
    "#### Implémentation et test du neurone\n",
    "\n",
    "La classe `Neurone` que nous allons implémenter représente un neurone individuel.\n",
    "La classe est munie des attributs suivants :\n",
    "\n",
    "- `poids` : un tableau des poids associés aux entrées du neurone ;\n",
    "- `biais` : un scalaire représentant le biais ajouté à la somme pondérée des entrées ;\n",
    "- `fonction_activation` : une chaîne de caractères qui spécifie la fonction d'activation à utiliser (`'sigmoïde'`, `'relu'`, etc.).\n",
    "\n",
    "et des méthodes suivantes :\n",
    "\n",
    "- `__init__(self, nbr_entrees, fonction_activation='sigmoïde')` : Initialise les poids, le biais et la fonction d'activation du neurone. Les poids sont initialisés avec des valeurs aléatoires comprises entre -0.1 et 0.1, le biais à une valeur aléatoire proche de 0. Quant à la fonction d'activation, elle est fixée par défaut avec la fonction sigmoïde.\n",
    "- `_appliquer_activation(self, x)` : Applique la fonction d'activation définie (sigmoïde, ReLU, etc.) à la somme pondérée et retourne la valeur activée.\n",
    "- `calculer_sortie(self, entrees)` : Calcule la somme pondérée des entrées, applique la fonction d'activation, et retourne la sortie du neurone.\n",
    "\n",
    "1. Implémentez la classe `Neurone`.\n",
    "2. Créez un neurone qui utilise une fonction d'activation sigmoïde et testez-le avec une des données d'entraînement comme l'indique le code suivant :\n",
    "\n",
    "   <div style=\"\n",
    "     padding: 5pt;\n",
    "     border-style: dashed;\n",
    "     border-width: 1px;\n",
    "     border-color: gray;\">\n",
    "\n",
    "   ```python\n",
    "   # Test d'un neurone avec une fonction d'activation sigmoïde\n",
    "   neurone_test_sigmoïde = Neurone(X_train.shape[1], fonction_activation='sigmoïde')\n",
    "   sortie_sigmoïde = neurone_test_sigmoïde.calculer_sortie(X_train[0])\n",
    "   print(\"Sortie du neurone (sigmoïde) :\", sortie_sigmoïde)\n",
    "   ```\n",
    "\n",
    "   </div>\n",
    "3. Créez un deuxième neurone qui utilise cette fois-ci une fonction d'activation ReLU. Qu'observez-vous ?\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88263953",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5722764",
   "metadata": {},
   "source": [
    "\n",
    "Dans ces deux premières étapes, nous avons préparé nos données et introduit le concept\n",
    "de neurone individuel avec une fonction d'activation (sigmoïde, etc.). Nous allons\n",
    "maintenant implémenter une **couche de neurones**, étape essentielle dans la création\n",
    "de notre réseau de neurones multicouche.\n",
    "\n",
    "### 3. Construction d'une couche de neurones\n",
    "\n",
    "Une couche de neurones est une **collection de neurones** qui fonctionnent en parallèle.\n",
    "En combinant les sorties de plusieurs neurones, la couche est capable de produire des\n",
    "représentations plus diversifiées des données qu'elle n'en a en entrée.\n",
    "\n",
    "Mathématiquement, pour une couche contenant $m$ neurones, chaque neurone $j$ produit\n",
    "une sortie $Z_j$ qui est calculée comme suit :\n",
    "\n",
    "$$\n",
    "Z_j = \\phi_j \\left( \\sum_{i=1}^{n} w_{ij} x_i + b_j \\right), \\quad \\forall j \\in [1, m]\n",
    "$$\n",
    "\n",
    "où :\n",
    "\n",
    "- $x_i$ représente les entrées de la couche ;\n",
    "- $w_{ij}$ est le poids de la connexion entre l'entrée $i$ et le neurone $j$ ;\n",
    "- $b_j$ est le biais du neurone $j$ ;\n",
    "- $\\phi_j$ est la fonction d'activation appliquée par le neurone $j$.\n",
    "\n",
    "Cette couche constitue un \"bloc\" de base dans un réseau de neurones. Pour réaliser cette\n",
    "structure, nous allons implémenter la classe `Couche` qui représente notre ensemble de\n",
    "neurones. Elle reçoit les entrées, les transmet à chaque neurone, puis collecte les\n",
    "sorties des neurones en une seule sortie. La classe `Couche` est munie de l'attribut\n",
    "suivant :\n",
    "\n",
    "- `neurones` : une liste d'objets de type `Neurone` dans laquelle chaque neurone possède ses propres poids et biais.\n",
    "\n",
    "et des méthodes suivantes :\n",
    "\n",
    "- `__init__(self, nbr_entrees, nbr_neurones, fonction_activation='sigmoïde')` : Crée la liste des `nbr_neurones` neurones de la couche, chacun ayant un nombre d'entrées `nbr_entrees` et une fonction d'activation associée.\n",
    "- `calculer_sorties(self, entrees)` : Passe les entrées à chaque neurone, collecte leurs sorties, et retourne la sortie de la couche sous forme de tableau (vous utiliserez `np.array()` pour réaliser cette opération).\n",
    "\n",
    "#### Implémentation et test d'une couche de neurones\n",
    "\n",
    "1. Implémentez la classe `Couche`.\n",
    "2. Créez et testez une couche de 10 neurones à fonction d'activation sigmoïde comme l'indique le code suivant :\n",
    "\n",
    "   <div style=\"\n",
    "     padding: 5pt;\n",
    "     border-style: dashed;\n",
    "     border-width: 1px;\n",
    "     border-color: gray;\">\n",
    "\n",
    "   ```python\n",
    "   # Test de la classe couche\n",
    "   couche_test = Couche(X_train.shape[1], 10, fonction_activation='sigmoïde')\n",
    "   sorties_couche = couche_test.calculer_sorties(X_train[0])\n",
    "   print(\"Sorties de la couche pour un exemple de données d'entraînement :\", sorties_couche)\n",
    "   ```\n",
    "\n",
    "   </div>\n",
    "3. Créez une couche avec 10 neurones à fonction d'activation ReLU et testez-la. Comparez les résultats avec ceux obtenus avec la couche précédente. Que remarquez-vous ?\n",
    "\n",
    "Une seule couche reste cependant insuffisante pour capturer des relations complexes. Pour aller plus loin, nous ajouterons des **couches cachées** dans les étapes suivantes afin de construire notre réseau multicouche.\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b6444",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ed4ad",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Construction d'un réseau de neurones multicouche\n",
    "\n",
    "Comme nous l'avons vu, un réseau de neurones multicouche (ou perceptron multicouche) est constitué d'une **couche d'entrée**, d'une ou plusieurs **couches cachées** et d'une **couche de sortie**. Chaque couche intermédiaire ou cachée permet d'extraire des caractéristiques de plus en plus abstraites des données d'entrée. C'est cette profondeur et cette capacité à capter des **relations complexes** entre les caractéristiques qui distinguent les réseaux de neurones multicouche des modèles linéaires de base.\n",
    "\n",
    "Le réseau multicouche fonctionne en **propagation avant** (ou *forward propagation*), c'est-à-dire que les données passent d'une couche à la suivante, chaque couche calculant puis transmettant ses sorties jusqu'à la couche finale.\n",
    "\n",
    "#### Architecture d'un réseau de neurones multicouche\n",
    "\n",
    "1. **Couche d'entrée** : elle reçoit les données d'entrée et chaque caractéristique est reliée à un neurone de la première couche.\n",
    "2. **Couches cachées** : ensemble de plusieurs couches de neurones interconnectés où chaque couche prend en entrée les sorties de la couche précédente. Chaque couche cachée aide à extraire des caractéristiques intermédiaires.\n",
    "3. **Couche de sortie** : elle génère la prédiction du modèle. Dans notre cas (classification binaire), elle se compose d'un seul neurone avec une fonction d'activation sigmoïde, de manière à fournir une probabilité comprise entre 0 et 1.\n",
    "\n",
    "#### Classe `ReseauDeNeurones`\n",
    "\n",
    "La classe `ReseauDeNeurones` que nous allons implémenter est composée de plusieurs couches successives, en commençant par les couches cachées, puis en terminant par une couche de sortie. Cette classe orchestre la propagation des données d'une couche à l'autre. Elle est munie de l'attribut suivant :\n",
    "\n",
    "- `couches` : une liste d'objets de type `Couche` qui constituent l'architecture du réseau. Chaque couche reçoit les sorties de la couche précédente.\n",
    "\n",
    "et des méthodes suivantes :\n",
    "\n",
    "- `__init__(self, nbr_entrees, structure_couches, fonction_activation='relu', fonction_activation_sortie='sigmoïde')` : Initialise le réseau avec les caractéristiques suivantes :\n",
    "  - `nbr_entrees` spécifie le nombre de caractéristiques en entrée.\n",
    "  - `structure_couches` est une liste où chaque élément indique le nombre de neurones dans une couche. Par exemple, `[30, 15, 1]` signifie deux couches cachées avec 30 neurones dans la première, 15 dans la suivante, et une couche de sortie avec 1 neurone.\n",
    "  - `fonction_activation` est utilisée pour toutes les couches cachées. Par défaut, nous utilisons la fonction `relu` qui est couramment employée pour ses avantages en termes de convergence.\n",
    "  - `fonction_activation_sortie` est la fonction d'activation appliquée à la couche de sortie. Ici, nous utilisons la fonction sigmoïde pour obtenir une sortie entre 0 et 1, compatible avec notre tâche de classification binaire.\n",
    "- `calculer_sortie(self, entrees)` fait passer les entrées à travers chaque couche en calculant la propagation avant, et retourne la sortie finale du réseau.\n",
    "\n",
    "#### Implémentation et test du réseau de neurones\n",
    "\n",
    "1. Implémentez la classe `ReseauDeNeurones`.\n",
    "2. Testez le réseau avec 2 couches cachées comprenant 30 neurones dans la première, 15 dans la suivante et suivie par une couche de sortie à 1 neurone.\n",
    "\n",
    "   <div style=\"\n",
    "     padding: 5pt;\n",
    "     border-style: dashed;\n",
    "     border-width: 1px;\n",
    "     border-color: gray;\">\n",
    "\n",
    "   ```python\n",
    "   # Test du réseau sur un échantillon de données\n",
    "   reseau_test = ReseauDeNeurones(X_train.shape[1], [30, 15, 1])\n",
    "   sortie_reseau = reseau_test.calculer_sortie(X_train[0])\n",
    "   print(\"Sortie du réseau pour un échantillon des données d'entraînement :\", sortie_reseau)\n",
    "   ```\n",
    "\n",
    "   </div>\n",
    "3. Testez à nouveau le réseau en considérant une couche cachée de 10 neurones suivie d'une couche de sortie à 1 neurone. Observez comment cela affecte la sortie du réseau pour l'exemple d'entraînement précédent.\n",
    "4. Testez d'autres configurations et décrivez leurs résultats.\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c13bbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43ef46",
   "metadata": {},
   "source": [
    "\n",
    "Vérifions avec la fonction d'évaluation suivante l'exactitude du réseau sur notre jeu de données :\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "def evaluer_reseau(reseau, X, y):\n",
    "    y_pred = [reseau.calculer_sortie(x)[0] > 0.5 for x in X]  # Prédictions binaires\n",
    "    return accuracy_score(y, y_pred)\n",
    "\n",
    "# Évaluation sur l'ensemble de test\n",
    "exactitude = evaluer_reseau(reseau_entrainement, X_test, y_test)\n",
    "print(\"Exactitude du réseau sur l'ensemble de test :\", exactitude)\n",
    "```\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01df049a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c5470",
   "metadata": {},
   "source": [
    "\n",
    "Nous avons maintenant un réseau capable de calculer une sortie, mais il manque un mécanisme d'apprentissage fondamental : celui qui permet au réseau de \"remonter\" ses erreurs de façon à ajuster les poids et les biais des neurones.\n",
    "\n",
    "Dans la deuxième partie, nous implémenterons l'algorithme de **rétropropagation** qui va permettre en ajustant correctement les paramètres du réseau de minimiser l'erreur entre les sorties obtenues et les sorties attendues. L'objectif final est d'optimiser la capacité de généralisation et d'apprentissage de notre réseau sur les tâches les plus variées.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Dans cette séance, vous avez :\n",
    "\n",
    "- implémenté en partie un perceptron multicouche sans utiliser de bibliothèques spécialisées ;\n",
    "- appliqué ce modèle incomplet à un problème de classification binaire.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
