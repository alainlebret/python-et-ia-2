{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c1e5db-b21e-4588-b942-3c76634e63b1",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: solid;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\n",
    "  border-radius: 10px;\">\n",
    "  \n",
    "# **Python et intelligence artificielle**\n",
    "\n",
    "# *Séance n°10 : Apprentissage par renforcement*\n",
    "\n",
    "</div>\n",
    "\n",
    "Dans cette séance nous continuons notre découverte des méthodes d'apprentissage automatique avec l'apprentissage par **renforcement**.\n",
    "\n",
    "## Dépendances\n",
    "\n",
    "Avant de commencer ce TP, assurez-vous d'avoir installé les bibliothèques *NumPy* et *Matplotlib*.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt, colors\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "L'apprentissage par **renforcement** est une méthode d'apprentissage automatique qui permet à des programmes informatiques, que nous appelons des \"agents\", d'apprendre à prendre des décisions optimales à travers des expériences. Imaginez un robot qui apprend à naviguer dans un labyrinthe ; à chaque intersection, il doit choisir entre tourner à gauche, à droite ou continuer tout droit. L'objectif est de trouver la sortie le plus rapidement possible. L'apprentissage par renforcement aide le robot à déterminer la meilleure action à prendre à chaque étape en fonction de sa position actuelle et de ses expériences passées.\n",
    "\n",
    "### Éléments de base\n",
    "\n",
    "- L'**agent** : C'est l'entité qui apprend à accomplir une tâche. Par exemple, un bras robotique qui apprend à déplacer des objets, un robot sommelier qui propose le vin à associer à un plat, une voiture autonome qui ménage sa batterie, un agent conversationnel qui cherche à satisfaire la demande d'un client, etc.\n",
    "- L'**environnement** : C'est le monde dans lequel l'agent opère. Pour un capteur dans une centrale nucléaire, l'environnement serait la centrale elle-même et les données qu'il mesure.\n",
    "- Les **états** : Ce sont les différentes situations ou configurations possibles de l'environnement. Pour un capteur, un état pourrait être la lecture actuelle de la température ou de la pression.\n",
    "- Les **actions** : Ce sont les différentes opérations que l'agent peut effectuer. Dans le cas de la robotique, cela pourrait être le déplacement d'un bras robotique ou l'ajustement d'un paramètre sur un capteur.\n",
    "- Les **récompenses** : Après chaque action, l'agent reçoit une \"récompense\" ou une \"punition\" qui lui indique si l'action était bénéfique ou non. Un capteur qui détecte un risque dans la centrale pourrait déclencher une action qui prévient un accident, ce qui serait une grande récompense.\n",
    "\n",
    "L'agent essaie différentes actions et apprend de ses erreurs et de ses succès. Au début, ses choix sont totalement aléatoires, mais au fil du temps, il commence à reconnaître les chemins qui mènent à de meilleures récompenses.\n",
    "\n",
    "Parmi les différents algorithmes d'apprentissage par renforcement, on trouve l'algorithme **Q-learning**.\n",
    "\n",
    "## 1. Mise en oeuvre de l'algorithme *Q-learning*\n",
    "\n",
    "L'algorithme *Q-learning* permet à un agent d'apprendre à prendre des décisions en calculant ce que l'on appelle une \"table de qualité\" (*Q-table* en anglais). Cette table aide l'agent à estimer quelles actions sont les plus prometteuses en ce qui concerne les récompenses futures.\n",
    "Le *Q-learning* est un algorithme dit \"sans modèle\", ce qui signifie qu'il n'a pas besoin de comprendre ou de modéliser l'environnement dans lequel il opère. Il apprend uniquement à partir des récompenses qu'il reçoit en conséquence de ses actions. C'est un peu comme si vous appreniez à trouver votre chemin dans une ville étrangère sans GPS ni plan : au lieu de cela, vous vous souvenez des rues qui vous ont mené à des destinations agréables et vous évitez celles qui vous ont conduit dans des impasses.\n",
    "\n",
    "Le *Q-learning* est particulièrement utile dans les situations où l'environnement est complexe et changeant, comme dans la gestion des opérations d'une centrale nucléaire, dans la navigation d'un robot dans un espace encombré ou à haut risque, et pourquoi pas d'un robot sommelier. Il permet à l'agent d'apprendre de ses erreurs et de s'adapter à de nouvelles situations sans avoir besoin de reprogrammation.\n",
    "\n",
    "### Comment fonctionne le *Q-learning* ?\n",
    "\n",
    "La *Q-table* est un tableau qui renseigne la récompense attendue pour chaque action dans chaque état. Chaque ligne de la table correspond à un état possible dans l'environnement, et chaque colonne correspond à une action possible que l'agent peut réaliser.\n",
    "Voici à quoi ressemble une *Q-table* pour un environnement simple avec 3 états et 2 actions possibles :\n",
    "\n",
    "|  États/Actions |  Action a1 |  Action a2 |\n",
    "|:--------------:|:----------:|:----------:|\n",
    "| État e1        | Q(e1, a1)  | Q(e1, a2)  |\n",
    "| État e2        | Q(e2, a1)  | Q(e2, a2)  |\n",
    "| État e3        | Q(e3, a1)  | Q(e3, a2)  |\n",
    "\n",
    "- `Q(e1, a1)` est la valeur de la récompense si l'agent choisit l'action a1 lorsqu'il est dans l'état e1.\n",
    "- `Q(e2, a2)` est la valeur de la récompense si l'agent choisit l'action a2 lorsqu'il est dans l'état e2, et ainsi de suite.\n",
    "\n",
    "L'algorithme est le suivant :\n",
    "\n",
    "1. **Initialisation** : On commence par créer la *Q-table* avec des valeurs initiales arbitraires.\n",
    "2. **Exploration** : L'agent explore l'environnement et, à chaque étape, met à jour la *Q-table* en fonction des récompenses obtenues. Au début, l'agent essaie des actions au hasard pour collecter des informations sur l'environnement.\n",
    "3. **Exploitation** : Avec le temps, l'agent commence à utiliser les informations accumulées dans la *Q-table* pour choisir les actions qui semblent offrir les meilleures récompenses futures.\n",
    "4. **Mise à jour de la Q-table** : Après chaque action, l'agent met à jour la *Q-table* en utilisant la formule de Bellman, qui intègre la récompense reçue et les récompenses futures estimées :\n",
    "\n",
    "    $$\n",
    "    Q_{\\text{nouveau}}(e, a) = Q(e, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(e', a') - Q(e, a) \\right]\n",
    "    $$\n",
    "\n",
    "    où :\n",
    "    - $Q(e, a)$ est la valeur actuelle de la *Q-table* pour l'état actuel $e$ et l'action $a$.\n",
    "    - $Q_{\\text{nouveau}}(e, a)$ est la nouvelle valeur mise à jour pour cet état et cette action.\n",
    "    - $\\alpha$ est le taux d'apprentissage, qui détermine à quel point les nouvelles informations influencent les anciennes. Une valeur plus élevée permet à l'agent d'apprendre plus rapidement, mais une valeur trop élevée peut rendre l'apprentissage instable. \n",
    "    - $r$ est la récompense reçue après avoir exécuté l'action $a$.\n",
    "    - $\\gamma$ est le facteur d'actualisation qui évalue l'importance des récompenses futures. Une valeur proche de 1 fera en sorte que l'agent valorise fortement les récompenses à long terme, alors que pour une valeur proche de 0 l'agent valorise les récompenses immédiates.\n",
    "    - $\\max_{a'} Q(e', a')$ est la valeur maximale de $Q$ pour le prochain état $e'$, en prenant en compte toutes les actions possibles $a'$. Cela représente la meilleure récompense future espérée.\n",
    "  \n",
    "> Remarque : vous pourrez avoir besoin de la fonction [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) pour récupérer $\\max_{a'} Q(e', a')$.\n",
    "\n",
    "### Stratégie $\\epsilon$-greedy\n",
    "\n",
    "La stratégie $\\epsilon$-greedy consiste à aider un algorithme à décider quand il est nécessaire d'essayer de nouvelles actions ou d'utiliser ce qu'il sait déjà. Autrement dit, c'est une stratégie qui va aider à choisir entre **exploration** de l'environnement et **exploitation** de celui-ci. Dans cette stratégie, le taux d'exploration $\\epsilon$ est un nombre compris entre 0 et 1 qui peut varier avec le temps, notamment dans le cas d'un environnement dynamique.\n",
    "\n",
    "L'algorithme est le suivant :\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```default\n",
    "Si un nombre aléatoire est inférieur à epsilon :\n",
    "    Choisir une action aléatoire parmi toutes les actions possibles.\n",
    "Sinon :\n",
    "    Choisir l'action qui a la plus haute valeur Q pour l'état actuel.\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "Dans sa version de base, les actions testées sont choisies au hasard, sans penser à leur potentiel, ce qui est un inconvénient.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Une première mise en oeuvre\n",
    "\n",
    "Voici deux premiers exercices qui n'ont pour seul intérêt que de vous entraîner à définir des états, des actions, une *Q-table* et de mettre en oeuvre la formule de Bellman afin de mettre à jour la table.\n",
    "\n",
    "### Exercice 1.1 : Le jeu des trois gobelets\n",
    "\n",
    "Imaginons un jeu où il y a trois gobelets retournés : **'A'**, **'B'** et **'C'**. Sous un de ces gobelets se trouve une récompense. Le but est de choisir le gobelet qui permet d'obtenir cette récompense.\n",
    "\n",
    "##### Environnement\n",
    "\n",
    "- **États** : Il n'y a qu'un seul état dans cet environnement, car il n'y a pas de séquence d'actions à prendre en compte.\n",
    "- **Actions** : Les actions possibles consistent à choisir le gobelet 'A', 'B' ou 'C'.\n",
    "- **Récompenses** : La récompense est de +1 si vous choisissez le bon gobelet, et 0 sinon.\n",
    "\n",
    "##### Règles\n",
    "\n",
    "- Après chaque choix, le jeu vous informe si vous avez choisi le bon gobelet ou non.\n",
    "- La position de la récompense ne change pas pendant une \"session\" d'apprentissage, mais peut changer entre les sessions afin d'éviter que l'agent n'apprenne simplement à toujours choisir le même gobelet.\n",
    "\n",
    "L'objectif est de coder un agent qui utilise le *Q-learning* pour déterminer le meilleur gobelet à choisir pour maximiser ses récompenses sur le long terme.\n",
    "\n",
    "##### 1. Initialisation\n",
    "\n",
    "Comme il n'y a qu'un seul état, la *Q-table* ne comporte qu'une seule ligne que nous appellerons `Q_values`. Nous la représentons par un dictionnaire dont les clés (actions) sont les noms des gobelets ('A', 'B' ou 'C') et les valeurs sont au départ à 0.\n",
    "Le gobelet contenant la récompense est choisi aléatoirement à l'aide de la fonction `np.random.choice()`. Le taux d'exploration est fixé à 0.1, le taux d'apprentissage à 0.1 et le nombre d'épisodes à 100.\n",
    "\n",
    "##### 2. Choix d'action\n",
    "\n",
    "Compléter la fonction `choose_action()` qui prend comme paramètre la table `Q_values` et `epsilon`, et qui applique une stratégie $\\epsilon$-greedy. La fonction retourne une action aléatoire ou celle qui présente la plus grande valeur dans `Q_values`.\n",
    "\n",
    "##### 3. Calcul des récompenses\n",
    "\n",
    "Compléter la fonction `get_reward()` qui à chaque tour prend comme paramètre l'action retournée par `choose_action()` et le nom du gobelet gagnant. La fonction retourne 1 si le gobelet choisi correspond au gobelet gagnant, 0 sinon.\n",
    "\n",
    "##### 4. Mise à jour des valeurs Q\n",
    "\n",
    "Compléter la fonction `update_Q_values()` qui met à jour la valeur Q pour l'action choisie en utilisant la récompense reçue. Cette fonction réalise la formule de Bellman dans laquelle nous considérerons que $\\gamma$ est nul.\n",
    "\n",
    "L'entraînement est répété 100 fois afin d'apprendre quel gobelet a la plus haute valeur Q, c'est-à-dire la plus grande chance d'offrir une récompense.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "# Initialisation des paramètres du Q-learning\n",
    "Q_values = {'A': 0, 'B': 0, 'C': 0}  # Valeurs Q pour chaque action\n",
    "reward_goblet = np.random.choice(['A', 'B', 'C'])  # Gobelet avec récompense\n",
    "epsilon = 0.1   # Taux d'exploration\n",
    "alpha = 0.1     # Taux d'apprentissage\n",
    "episodes = 100  # Nombre d'épisodes pour l'apprentissage\n",
    "\n",
    "# Fonction pour choisir une action\n",
    "def choose_action(Q_values, epsilon):\n",
    "    # À compléter pour choisir une action en utilisant epsilon-greedy\n",
    "    pass\n",
    "\n",
    "# Fonction pour obtenir une récompense\n",
    "def get_reward(action, reward_goblet):\n",
    "    # À compléter pour obtenir la récompense quant à l'action\n",
    "    pass\n",
    "\n",
    "# Fonction pour mettre à jour les valeurs Q en fonction de l'action, la réaction et la valeur de alpha\n",
    "def update_Q_values(Q_values, action, reward, alpha):\n",
    "    # À compléter pour mettre à jour la table Q\n",
    "    pass\n",
    "\n",
    "# Processus d'apprentissage\n",
    "for episode in range(episodes):\n",
    "    action = choose_action(Q_values, epsilon)\n",
    "    reward = get_reward(action, reward_goblet)\n",
    "    update_Q_values(Q_values, action, reward, alpha)\n",
    "\n",
    "# Affichage des valeurs Q apprises\n",
    "print(\"Valeurs Q : \", Q_values)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "- `choose_action()` implémente la stratégie $\\epsilon$-greedy ;\n",
    "- `get_reward()` retourne 1 si le gobelet choisi est le bon, 0 sinon.\n",
    "- `update_Q_values()` met à jour la valeur Q en utilisant un $\\gamma$ nul (car il n'y a pas d'état futur).\n",
    "\n",
    "### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4fa2a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab7cdd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Exercice 1.2\n",
    "\n",
    "Imaginons un robot se déplaçant dans un couloir avec 5 positions possibles (0 à 4), où 0 est le point de départ et 4 est le point d'arrivée avec une récompense.\n",
    "\n",
    "#### Environnement\n",
    "\n",
    "- **États** : Ce sont les positions dans le couloir (de 0 à 4).\n",
    "- **Actions** : Les actions possibles consistent à aller à gauche (0) ou à droite (1), sauf aux extrémités du couloir.\n",
    "- **Récompenses** : La récompense est de +1 si le robot atteint la position 4, et 0 sinon.\n",
    "\n",
    "#### 1. Initialisation\n",
    "\n",
    "La *Q-table* est une matrice de 5 lignes (5 états pour 5 positions dans le couloir) et 2 colonnes (2 actions, 0 pour 'gauche' et 1 pour 'droite'). Le taux d'apprentissage est fixé à 0.1, le facteur de remise à 0.9 et le taux d'exploration à 0.1.\n",
    "\n",
    "#### 2. Choix d'action\n",
    "\n",
    "Complétez la fonction `choose_action()` qui prend comme paramètre l'état, et qui applique une stratégie $\\epsilon$-greedy. La fonction retourne une action aléatoire ou celle qui présente la plus grande valeur dans `Q_table`.\n",
    "\n",
    "#### 3. Déterminer le nouvel état\n",
    "\n",
    "Complétez la fonction `get_next_state()` qui prend comme paramètre l'action à réaliser et retourne le nouvel état. Cette fonction doit s'assurer que le nouvel état ne dépasse pas les limites.\n",
    "\n",
    "#### 3. Calcul des récompenses\n",
    "\n",
    "Complétez la fonction `get_reward()` qui prend comme paramètre le nouvel état retourné par `get_next_state()`. La fonction retourne 1 si l'état correspond à la position 4, 0 sinon.\n",
    "\n",
    "##### 4. Mise à jour des valeurs Q\n",
    "\n",
    "Complétez la fonction `update_Q_table()` qui met à jour la valeur Q pour l'action choisie en utilisant la récompense reçue. Cette fonction réalise la formule de Bellman.\n",
    "\n",
    "L'apprentissage est répété 100 fois.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "  \n",
    "```python\n",
    "# Paramètres de l'algorithme Q-learning\n",
    "alpha = 0.1    # Taux d'apprentissage\n",
    "gamma = 0.9    # Facteur de remise\n",
    "epsilon = 0.1  # Taux d'exploration\n",
    "num_episodes = 100  # Nombre total d'épisodes pour l'entraînement\n",
    "\n",
    "# Initialisation de la table Q\n",
    "num_states = 5\n",
    "num_actions = 2  # 0 : gauche, 1 : droite\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Fonction pour choisir une action\n",
    "def choose_action(state):\n",
    "    # À compléter pour choisir une action en utilisant epsilon-greedy\n",
    "    pass\n",
    "\n",
    "# Fonction pour déterminer le nouvel état suite à l'action\n",
    "def get_next_state(action):\n",
    "    # À compléter pour déterminer le nouvel état suite à l'action\n",
    "    pass\n",
    "\n",
    "# Fonction pour obtenir une récompense\n",
    "def get_reward(next_state):\n",
    "    # À compléter pour obtenir la récompense quant à l'état suivant\n",
    "    pass\n",
    "\n",
    "# Fonction pour mettre à jour la table Q\n",
    "def update_Q(state, action, reward, next_state):\n",
    "    # À compléter pour mettre à jour la table Q\n",
    "    pass\n",
    "\n",
    "# Boucle d'apprentissage\n",
    "for episode in range(num_episodes):\n",
    "    state = 0  # On commence toujours à la position 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state)\n",
    "        next_state = get_next_state(action)\n",
    "        reward = get_reward(next_state)\n",
    "        update_Q(state, action, reward, next_state)\n",
    "        state = next_state\n",
    "        \n",
    "        if state == num_states - 1:\n",
    "            done = True  # L'objectif est atteint\n",
    "\n",
    "# Tester l'agent après l'entraînement\n",
    "state = 0\n",
    "while state != num_states - 1:\n",
    "    action = np.argmax(Q[state])\n",
    "    state += 1 if action == 1 else -1\n",
    "    print(f\"L'agent se déplace vers {'la droite' if action == 1 else 'la gauche'}, nouvelle position: {state}\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a2cd4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91899cc6",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2. Application à la simulation de navigation d'un robot\n",
    "\n",
    "Dans cet exercice plus avancé, nous allons simuler un robot qui doit naviguer dans un environnement afin d'atteindre un objectif tout en évitant des obstacles et des zones radioactives.\n",
    "\n",
    "### Contexte\n",
    "\n",
    "Nous supposons que le robot est équipé d'un capteur unifié permettant à la fois de détecter les obstacles, les zones radioactives et les limites de l'environnement. L'objectif est de naviguer de son point de départ à un point d'arrivée spécifié sans entrer en collision avec les obstacles et en évitant les zones radioactives.\n",
    "\n",
    "### Environnement\n",
    "\n",
    "L'environnement est une grille de taille 5$\\times$5, avec des obstacles statiques et des zones irradiées. Le robot peut recevoir des informations de son capteur indiquant la distance aux obstacles et aux zones radioactives les plus proches dans chaque direction.\n",
    "\n",
    "### Règles\n",
    "\n",
    "- Le robot peut se déplacer dans quatre directions: haut, bas, gauche, droite.\n",
    "- Les obstacles et les zones radioactives sont fixés aléatoirement au départ et ne changent pas de position en cours de simulation.\n",
    "- Si le robot entre en collision avec un obstacle, une pénalité de -50 est attribuée.\n",
    "- Si le robot pénètre dans une zone radioactive, une pénalité de -100 est attribuée.\n",
    "- Atteindre l'objectif donne une récompense de +100.\n",
    "- Le capteur a une portée limitée et ne peut détecter des obstacles et des zones radioactives que dans un certain rayon.\n",
    "\n",
    "### Implémentation avec des classes\n",
    "\n",
    "Nous allons définir plusieurs classes pour représenter l'environnement, le robot et le capteur :\n",
    "\n",
    "- Classe `Environment` qui représente la grille et gére les positions des obstacles, des zones radioactives et de l'objectif à atteindre.\n",
    "- Classe `Robot` qui représente notre agent et possède une méthode pour choisir une action basée sur les entrées de son capteur et une méthode pour mettre à jour sa position dans l'environnement.\n",
    "- Classe `Sensor` qui simule le capteur du robot et fournit des informations sur son environnement.\n",
    "- Classe `Simulation` qui orchestre l'interaction entre le robot et l'environnement, et permet de visualiser les résultats.\n",
    "\n",
    "#### Exercice 2.1 : création de la classe `Environment`\n",
    "\n",
    "##### 1. Initialisation de l'environnement\n",
    "\n",
    "La classe `Environment` est munie des attributs `size` (taille d'un côté de la grille carrée), `obstacles` (un ensemble / `set` contenant les obstacles), `radioactive_zones` (un ensemble / `set` contenant les zones radioactives) et `target_position` (la position de la cible).\n",
    "\n",
    "Les obstacles et les zones radioactives doivent être placés aléatoirement dans la grille, en veillant à ce qu'ils ne se trouvent pas sur la position de départ du robot (0, 0) ni sur la position de la cible à atteindre.\n",
    "\n",
    "Le contructeur prend les paramètres `size`, `obstacle_count` (nombre d'obstacles), `radioactive_count` (nombre de zones radioactives) et `target_position`, et il appelle les méthodes privées `_generate_obstacles()` et `_generate_radioactive_zones()` afin de peupler l'environnement avec le nombre spécifié d'obstacles et de zones radioactives. \n",
    "\n",
    "##### 2. Génération des obstacles et des zones radioactives\n",
    "\n",
    "Implémentez les méthodes privées `_generate_obstacles()` et `_generate_radioactive_zones()` pour peupler l'environnement avec le nombre spécifié d'obstacles et de zones radioactives. Ces méthodes sont appelées depuis le constructeur. Assurez-vous que les obstacles et les zones radioactives ne se chevauchent pas et ne sont pas placés sur la position initiale ou la position de la cible.\n",
    "\n",
    "##### 3. Détection des bords, des obstacles et des zones radioactives\n",
    "\n",
    "- La méthode `is_border()` doit déterminer si une position donnée est sur le bord de la grille.\n",
    "- Les méthodes `is_obstacle()` et `is_radioactive()` doivent vérifier si une position donnée contient un obstacle ou une zone radioactive, respectivement.\n",
    "\n",
    "##### 4. Affichage de l'environnement\n",
    "\n",
    "La méthode `display()` doit créer et retourner une représentation de l'environnement sous forme de grille. Les cases vides sont marquées par 0, les obstacles par -1, les zones radioactives par -2, et la position de l'objectif par 1.\n",
    "\n",
    "##### 5. Test de la classe\n",
    "\n",
    "Testez la classe `Environment` à l'aide du code fourni plus bas.\n",
    "\n",
    "##### Consignes supplémentaires\n",
    "\n",
    "Utilisez la bibliothèque *Numpy* pour générer des nombres aléatoires lors de la création des obstacles et des zones radioactives.\n",
    "Veillez à ce que la position de la cible soit toujours accessible et ne soit pas bloquée par un obstacle ou une zone radioactive.\n",
    "Assurez-vous que la méthode `display()` renvoie une grille qui reflète correctement l'état actuel de l'environnement.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "class Environment:\n",
    "    \"\"\"Représente l'environnement dans lequel le robot se déplace.\"\"\"\n",
    "\n",
    "    def __init__(self, size, obstacle_count, radioactive_count, target_position):\n",
    "        \"\"\"\n",
    "        Initialise l'environnement en générant les obstacles et les zones radioactives.\n",
    "\n",
    "        :param grid_size: Taille de la grille (int).\n",
    "        :param num_obstacles: Nombre d'obstacles à générer.\n",
    "        :param num_radioactive_zones: Nombre de zones radioactives à générer.\n",
    "        :param target_pos: Position de la cible (tuple).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _generate_obstacles(self, obstacle_count):\n",
    "        \"\"\"Génère les obstacles aléatoirement dans l'environnement.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _generate_radioactive_zones(self, radioactive_count):\n",
    "        \"\"\"Génère les zones radioactives aléatoirement dans l'environnement.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def is_border(self, position):\n",
    "        \"\"\"Teste si la position est sur un bord de la grille.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def is_obstacle(self, position):\n",
    "        \"\"\"Teste si la position est un obstacle.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def is_radioactive(self, position):\n",
    "        \"\"\"Teste si la position est une zone radioactive.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_target_position(self):\n",
    "        \"\"\"Renvoie la position de la cible.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Affiche une grille représentant l'environnement.\"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "- `_generate_obstacles()` et `_generate_radioactive_zones()` génèrent des positions aléatoires pour les obstacles et les zones radioactives, en évitant la position de départ et celle de l'objectif.\n",
    "- `display()` crée une représentation de la grille.\n",
    "\n",
    "##### Exemple d'utilisation\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "size = 5 # grille de 5x5\n",
    "obstacle_count = 3\n",
    "radioactive_count = 2\n",
    "target_position = (size - 1, size - 1)\n",
    "\n",
    "env = Environment(size, obstacle_count, radioactive_count, target_position)\n",
    "print(env.display())\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Test de la classe `Environment`\n",
    "\n",
    "Assurez-vous que la classe `Environment` est bien définie dans une des cellules précédentes.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "# Exemple de tests pour la classe Environment.\n",
    "\n",
    "# Création de l'environnement avec des paramètres spécifiques\n",
    "size = 5\n",
    "obstacle_count = 2\n",
    "radioactive_count = 2\n",
    "target_position = (4, 4)\n",
    "env = Environment(size, obstacle_count, radioactive_count, target_position)\n",
    "\n",
    "# Vérifier que la taille de l'environnement est correcte\n",
    "assert env.size == size, f\"Taille attendue: {size}, obtenue: {env.size}\"\n",
    "\n",
    "# Vérifier que le nombre d'obstacles est correct\n",
    "assert len(env.obstacles) == obstacle_count, f\"Nombre d'obstacles attendu: {obstacle_count}, obtenu: {len(env.obstacles)}\"\n",
    "\n",
    "# Vérifier que le nombre de zones radioactives est correct\n",
    "assert len(env.radioactive_zones) == radioactive_count, f\"Nombre de zones radioactives attendu: {radioactive_count}, obtenu: {len(env.radioactive_zones)}\"\n",
    "\n",
    "# Vérifier que la position de l'objectif est correcte\n",
    "assert env.target_position == target_position, f\"Position de l'objectif attendue: {target_position}, obtenue: {env.target_position}\"\n",
    "\n",
    "# Vérifier que la méthode display renvoie une matrice de la bonne taille\n",
    "display = env.display()\n",
    "assert isinstance(display, list), \"La méthode display doit retourner une liste.\"\n",
    "assert all(isinstance(row, list) for row in display), \"Chaque élément de la liste renvoyée par display() doit être une liste.\"\n",
    "assert all(len(row) == size for row in display), \"Chaque sous-liste renvoyée par display() doit avoir une longueur égale à size.\"\n",
    "\n",
    "# Vérifier que les coins de la grille font partie de la bordure\n",
    "corners = [(0, 0), (0, 4), (4, 0), (4, 4)]\n",
    "for corner in corners:\n",
    "    assert env.is_border(corner), f\"Le coin en position {corner} devrait être détecté comme bordure.\"\n",
    "\n",
    "# Le centre de la grille ne peut pas faire partie de la bordure dans notre cas\n",
    "center = (2, 2)\n",
    "assert not env.is_border(center), f\"Le centre {center} ne devrait pas être détecté comme bordure.\"\n",
    "\n",
    "# Vérifier que les bords sont détectés\n",
    "borders = [(0, 1), (0, 2), (0, 3), (1, 0), (1, 4), (2, 0), (2, 4), (3, 0), (3, 4), (4, 1), (4, 2), (4, 3)]\n",
    "for border in borders:\n",
    "    assert env.is_border(border), f\"La cellule en position {corner} devrait être détecté comme bordure.\"\n",
    "\n",
    "# Vérifier que les obstacles et les zones radioactives sont correctement placés dans la matrice renvoyée par display\n",
    "for y, row in enumerate(display):\n",
    "    for x, cell in enumerate(row):\n",
    "        if (x, y) in env.obstacles:\n",
    "            assert cell == -1, f\"Obstacle attendu en position {(x, y)}, mais obtenu {cell}\"\n",
    "        elif (x, y) in env.radioactive_zones:\n",
    "            assert cell == -2, f\"Zone radioactive attendue en position {(x, y)}, mais obtenu {cell}\"\n",
    "        elif (x, y) == env.target_position:\n",
    "            assert cell == 1, f\"Objectif attendu en position {(x, y)}, mais obtenu {cell}\"\n",
    "        else:\n",
    "            assert cell == 0, f\"Case vide attendue en position {(x, y)}, mais obtenu {cell}\"\n",
    "\n",
    "print(\"Tous les tests pour la classe Environment ont réussi.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Exercice 2.2 : création de la classe `Sensor`\n",
    "\n",
    "Vous allez développer une classe `Sensor` qui simule le capteur du robot. Ce capteur permet de détecter les obstacles, les zones radioactives et les bords de l'environnement dans lequel le robot se déplace.\n",
    "\n",
    "##### 1. Initialisation du capteur\n",
    "\n",
    "La classe `Sensor` possède un attribut `range_detection` qui définit la portée de détection du capteur.\n",
    "La portée de détection est la distance maximale à laquelle le capteur peut détecter un élément dans l'environnement.\n",
    "\n",
    "##### 2. Détection dans l'environnement\n",
    "\n",
    "Implémentez la méthode `detect()` qui prend en paramètres une position (sous forme de tuple de coordonnées) et un environnement (instance de la classe `Environment`). La méthode doit retourner un dictionnaire appelé `readings` dont les clés `'up'`, `'down'`, `'left'`, et `'right'` correspondent aux directions dans lesquelles le capteur effectue ses détections, et les valeurs correspondent au type de case :\n",
    "\n",
    "- Si un obstacle est détecté dans une direction, la valeur correspondante dans le dictionnaire `readings` doit être -1.\n",
    "- Si une zone radioactive est détectée, la valeur doit être -2.\n",
    "- Si le bord de l'environnement est détecté, la valeur doit être -3.\n",
    "- Si rien n'est détecté dans la portée du capteur pour une direction donnée, la valeur doit rester 0.\n",
    "\n",
    "Pour chaque direction, le capteur doit sonder l'environnement à partir de la position donnée, jusqu'à atteindre sa portée maximale ou rencontrer un élément à détecter.\n",
    "\n",
    "##### 3. Calcul de la position du capteur\n",
    "\n",
    "La méthode privée `_probe_position()` doit calculer et retourner la position de la case sondée par le capteur en fonction de la direction et de la distance spécifiées. Cette méthode est utilisée par `detect()` pour déterminer la position à sonder dans l'environnement.\n",
    "\n",
    "##### Consignes supplémentaires\n",
    "\n",
    "- Assurez-vous que la méthode `detect()` vérifie correctement les conditions de l'environnement en utilisant les méthodes `is_obstacle()`, `is_radioactive()`, et `is_border()` fournies par l'instance de `Environment`.\n",
    "- La détection doit s'arrêter immédiatement si un élément est détecté ou si la cellule sondée est au bord de l'environnement, quelle que soit la portée du capteur.\n",
    "- La méthode `_probe_position()` est privée et n'est utilisée qu'en interne par la classe `Sensor`.\n",
    "\n",
    "##### Bonus\n",
    "\n",
    "Si vous avez terminé l'implémentation de base, essayez d'ajouter une fonctionnalité supplémentaire grâce à laquelle le capteur peut détecter plusieurs éléments dans une direction donnée et retourner une liste des détections au lieu d'une seule valeur.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "class Sensor:\n",
    "    \"\"\"Simule un capteur de détection des obstacles, des zones radioactives et des bords.\"\"\"\n",
    "\n",
    "    def __init__(self, range_detection):\n",
    "        \"\"\"\n",
    "        Initialise le capteur avec sa distance de couverture.\n",
    "\n",
    "        :param range_detection: Distance de couverture du capteur.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def detect(self, position, environment):\n",
    "        \"\"\"\n",
    "        Renvoie les éléments détectés dans les 4 directions.\n",
    "\n",
    "        :param position: Position courante du robot.\n",
    "        :param environment: Instance de l'environnement.\n",
    "        :return: Dictionnaire contenant les éléments détectés.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _probe_position(self, position, direction, distance):\n",
    "        \"\"\"Calcule la position dans une direction donnée et à une distance donnée.\"\"\"\n",
    "        pass\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "- `detect()` : Pour chaque direction, le capteur sonde jusqu'à sa portée maximale ou jusqu'à ce qu'il détecte un élément.\n",
    "\n",
    "##### Exemple d'utilisation\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "sensor = Sensor(range_detection=3)\n",
    "robot_position = (0, 0)  # Position hypothétique du robot\n",
    "\n",
    "# Supposons que l'environnement 'env' a été créé précédemment\n",
    "sensor_readings = sensor.detect(robot_position, env)\n",
    "print(sensor_readings)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Test de la classe `Sensor`\n",
    "\n",
    "Assurez-vous que les classes `Environment` et `Sensor` sont bien définies dans les cellules précédentes.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "# Exemple de tests pour la classe Sensor.\n",
    "size = 5\n",
    "obstacle_count = 2\n",
    "radioactive_count = 2\n",
    "goal_position = (4, 4)\n",
    "env = Environment(size, obstacle_count, radioactive_count, goal_position)\n",
    "\n",
    "# Création d'un capteur\n",
    "sensor = Sensor(range_detection=1)\n",
    "\n",
    "# Position de test pour le robot\n",
    "robot_position = (2, 2)\n",
    "\n",
    "# Test de la détection des obstacles et des zones radioactives\n",
    "# Nous plaçons manuellement des obstacles et des zones radioactives pour ce test\n",
    "env.obstacles = [(1, 2), (3, 2)]  # Gauche et droite du robot\n",
    "env.radioactive_zones = [(2, 1), (2, 3)]  # Haut et bas du robot\n",
    "\n",
    "# Détecter les obstacles et les zones radioactives\n",
    "detections = sensor.detect(robot_position, env)\n",
    "\n",
    "# Vérifier que les obstacles sont correctement détectés\n",
    "expected_obstacle_detections = {'left': -1, 'right': -1, 'up': -2, 'down': -2}\n",
    "for direction, detection in expected_obstacle_detections.items():\n",
    "    assert detections[direction] == detection, f\"Obstacle mal détecté à {direction}: attendu {detection}, obtenu {detections[direction]}\"\n",
    "\n",
    "# Vérifier que les zones radioactives sont correctement détectées\n",
    "expected_radioactive_detections = {'left': 0, 'right': 0, 'up': -2, 'down': -2}\n",
    "for direction, detection in expected_radioactive_detections.items():\n",
    "    # Nous espérons des zones radioactives uniquement vers le haut et vers le bas\n",
    "    if direction in ['up', 'down']:\n",
    "        assert detections[direction] == detection, f\"Zone radioactive mal détectée à {direction}: attendu {detection}, obtenu {detections[direction]}\"\n",
    "\n",
    "# Test de la détection des limites de l'environnement\n",
    "border_position = (1, 1)  # Position du robot près d'un bord de l'environnement\n",
    "detections = sensor.detect(border_position, env)\n",
    "\n",
    "# Vérifier que les limites sont correctement détectées\n",
    "expected_border_detections = {'left': -3, 'right': -2, 'up': -3, 'down': -1}\n",
    "for direction, detection in expected_border_detections.items():\n",
    "    assert detections[direction] == detection, f\"Limite mal détectée à {direction}: attendu {detection}, obtenu {detections[direction]}\"\n",
    "\n",
    "print(\"Tous les tests pour la classe Sensor ont réussi.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Exercice 2.3 : création de la classe `Robot`\n",
    "\n",
    "Cette classe représente l'agent qui apprend à naviguer dans l'environnement.\n",
    "\n",
    "##### Attributs\n",
    "\n",
    "Les attributs du robot sont les suivants :\n",
    "\n",
    "- `actions` est une liste des actions possibles que le robot peut prendre (`'up'`, `'down'`, `'left'`, `'right'`) ;\n",
    "- `Q_table` est un dictionnaire vide qui stocke les valeurs Q pour chaque paire (état, action) ;\n",
    "- `sensor` est une instance de la classe `Sensor` ;\n",
    "- `position` est la position courante du robot dans l'environnement et est initialisée à (0, 0) au départ ;\n",
    "- `environment` est l'environnement dans lequel le robot évolue et doit être défini avant que le robot ne puisse agir ;\n",
    "- `verbose` est un booléen qui active l'affichage de détails sur les actions et les positions du robot lorsqu'il est à `True`.\n",
    "\n",
    "##### Méthodes\n",
    "\n",
    "Les méthodes à implémenter sont les suivantes :\n",
    "\n",
    "- `__init__()` initialise le robot avec une position de départ et un mode verbeux optionnel ;\n",
    "- `set_environment()` attribue un environnement au robot ;\n",
    "- `reset()` réinitialise la position du robot à sa position de départ ;\n",
    "- `set_sensor()` attribue un capteur au robot ;\n",
    "- `choose_action()` sélectionne une action à partir de l'état actuel à l'aide d'une stratégie $\\epsilon$-greedy ;\n",
    "- `update_Q_table()` met à jour la table Q en utilisant l'équation de Bellman ;\n",
    "- `take_action()` fait prendre une action au robot et retourne la récompense associée à cette action.\n",
    "\n",
    "##### Consignes supplémentaires\n",
    "\n",
    "- Lorsque le robot prend une action, il doit vérifier si la nouvelle position est un obstacle, une zone radioactive, ou l'objectif. Les récompenses sont attribuées comme suit :\n",
    "  - **-50** pour un obstacle ;\n",
    "  - **-100** pour une zone radioactive ;\n",
    "  - **100** pour l'objectif ;\n",
    "  - **-1** si le robot tente de sortir de la grille ;\n",
    "  - **0** pour tout autre déplacement valide.\n",
    "- Si le robot tente de se déplacer en dehors de la grille, il doit rester sur place et recevoir une pénalité.\n",
    "- La méthode `take_action()` doit également gérer les cas où l'environnement n'est pas défini.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "class Robot:\n",
    "    \"\"\"Représente le robot qui apprend à se déplacer dans son environnement.\"\"\"\n",
    "\n",
    "    def __init__(self, start_position=(0, 0), verbose=False):\n",
    "        \"\"\"\n",
    "        Initialise le robot.\n",
    "\n",
    "        :param start_position: Position de départ du robot.\n",
    "        :param verbose: Activation du mode verbeux (débogage) si True.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_environment(self, environment):\n",
    "        \"\"\"Associe un environnement au robot.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Replace le robot à sa position de départ.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def set_sensor(self, sensor):\n",
    "        \"\"\"Associe un capteur au robot.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Sélectionne une action à l'aide de la stratégie epsilon-greedy.\n",
    "\n",
    "        :param state: État courant du robot.\n",
    "        :param epsilon: Taux d'exploration.\n",
    "        :return: Action choisie.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_Q_table(self, old_state, action, reward, new_state, alpha, gamma):\n",
    "        \"\"\"\n",
    "        Met à jour la table Q à l'aide de l'équation de Bellman.\n",
    "\n",
    "        :param old_state: État précédent.\n",
    "        :param action: Action prise.\n",
    "        :param reward: Récompense reçue.\n",
    "        :param new_state: Nouvel état.\n",
    "        :param alpha: Taux d'apprentissage.\n",
    "        :param gamma: Remise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def take_action(self, action):\n",
    "        \"\"\"\n",
    "        Réalise l'action choisie et renvoie la récompense associée.\n",
    "\n",
    "        :param action: Action à exécuter.\n",
    "        :return: Récompense obtenue.\n",
    "        \"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "##### Exemple d'utilisation\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "env = Environment(size=5, obstacle_count=2, radioactive_count=3, target_position=(4, 4))\n",
    "robot = Robot()\n",
    "robot.set_environment(env)\n",
    "\n",
    "old_state = robot.position\n",
    "action = robot.choose_action(old_state, epsilon=0.1)\n",
    "reward = robot.take_action(action)\n",
    "new_state = robot.position\n",
    "robot.update_Q_table(old_state, action, reward, new_state, alpha=0.1, gamma=0.9)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "- `choose_action()` sélectionne une action selon la stratégie $\\epsilon$-greedy ;\n",
    "- `update_Q_table()` met à jour la table Q selon l'équation de Bellman ;\n",
    "- `take_action()` effectue l'action choisie, met à jour la position du robot et calcule la récompense.\n",
    "\n",
    "#### Test de la classe `Robot`\n",
    "\n",
    "Assurez-vous que les classes `Environment`, `Sensor` et `Robot` sont bien définies dans les cellules précédentes.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "size = 5\n",
    "obstacle_count = 2\n",
    "radioactive_count = 2\n",
    "target_position = (4, 4)\n",
    "env = Environment(size, obstacle_count, radioactive_count, target_position)\n",
    "\n",
    "# Création d'un robot avec son capteur unifié et associé à l'environnement\n",
    "sensor = Sensor(range_detection=1)\n",
    "robot = Robot(start_position=(0, 0), verbose=True)\n",
    "robot.set_environment(env)\n",
    "robot.set_sensor(sensor)\n",
    "\n",
    "# Test de la réinitialisation du robot\n",
    "robot.reset()\n",
    "assert robot.position == (0, 0), \"La position du robot devrait être réinitialisée à (0, 0).\"\n",
    "\n",
    "# Test de la sélection d'action epsilon-greedy\n",
    "epsilon = 0.1  # 10% de chance de choisir une action aléatoire\n",
    "action = robot.choose_action(robot.position, epsilon)\n",
    "assert action in robot.actions, \"L'action choisie doit être parmi les actions possibles.\"\n",
    "\n",
    "# Assurez-vous qu'il n'y a pas de zone radioactive à la position (0, 0)\n",
    "assert not env.is_radioactive((0, 0)), \"Il ne devrait pas y avoir de zone radioactive à la position initiale pour ce test.\"\n",
    "\n",
    "# Placez un obstacle à la position (0, 1) et vérifiez que is_obstacle() renvoie True\n",
    "env.obstacles = [(0, 1)]\n",
    "assert env.is_obstacle((0, 1)), \"La position (0, 1) devrait être un obstacle.\"\n",
    "\n",
    "# Maintenant, effectuez le test de prise d'action avec l'obstacle\n",
    "reward = robot.take_action('right')\n",
    "assert reward == -50, \"La récompense devrait être -50 pour avoir frappé un obstacle.\"\n",
    "\n",
    "# Place une zone radioactive à la position (0, 2) et vérifie que is_radioactive() renvoie True\n",
    "env.radioactive_zones = [(0, 2)]\n",
    "assert env.is_radioactive((0, 2)), \"La position (0, 2) devrait être radioactive.\"\n",
    "\n",
    "# Effectuons le test de prise d'action avec la zone radioactive\n",
    "robot.reset()\n",
    "reward = robot.take_action('right')\n",
    "reward = robot.take_action('right')\n",
    "print(f\"Récompense : {reward}\")\n",
    "assert reward == -100, \"La récompense devrait être -100 pour être entré dans une zone radioactive.\"\n",
    "\n",
    "# Maintenant, rendons nous sur l'objectif\n",
    "reward = robot.take_action('right')\n",
    "reward = robot.take_action('right')\n",
    "reward = robot.take_action('down')\n",
    "reward = robot.take_action('down')\n",
    "reward = robot.take_action('down')\n",
    "reward = robot.take_action('down')\n",
    "print(f\"Récompense : {reward}\")\n",
    "assert reward == 100, \"La récompense devrait être 100 pour avoir atteint l'objectif.\"\n",
    "\n",
    "print(\"Tous les tests pour la classe Robot ont réussi.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Exercice 2.4 : création de la classe `Simulation`\n",
    "\n",
    "Dans la classe `Simulation`, vous allez intégrer l'ensemble des composants qui interagissent afin de simuler votre robot qui se déplace dans son environnement. Voici un guide étape par étape pour vous aider à comprendre et à coder cette classe.\n",
    "\n",
    "##### 1. Initialisation de la classe\n",
    "\n",
    "Le constructeur `__init__()` initialise le robot et son environnement, ainsi que le capteur. Il prend comme paramètres :\n",
    "\n",
    "- `robot` une instance de la classe `Robot` ;\n",
    "- `environment` une instance de la classe `Environment` ;\n",
    "- `sensor` une instance de la classe `Sensor` ;\n",
    "- `verbose` un booléen qui active l'affichage de messages détaillés pendant la simulation lorsqu'il est à `True` (il est à `False` par défaut).\n",
    "\n",
    "##### 2. Entraînement du robot\n",
    "\n",
    "La méthode `train()` est utilisée pour entraîner le robot à naviguer dans l'environnement. Elle prend comme paramètres :\n",
    "\n",
    "- `episodes` : le nombre total d'épisodes d'entraînement ;\n",
    "- `max_steps_per_episode` : le nombre maximum de pas que le robot peut réaliser pendant un épisode ;\n",
    "- `epsilon` : le taux d'exploration qui contrôle le compromis entre exploration et exploitation ;\n",
    "- `alpha` : le taux d'apprentissage ;\n",
    "- `gamma` : le facteur de remise.\n",
    "\n",
    "##### 3. Trouver le meilleur chemin\n",
    "\n",
    "- La méthode `find_best_path()` ne prend aucun paramètre. Elle réinitialise la position du robot et suit le meilleur chemin qu'il a appris jusqu'à ce qu'il atteigne sa cible ou qu'une condition d'arrêt soit rencontrée. Cette méthode est appelée par `visualize_best_path()`.\n",
    "- Les méthodes `visualize()` et `visualize_best_path()` sont fournies afin de vous aider à visualiser l'environnement et le chemin que le robot prend. Les cases blanches représentent les espaces libres, les cases rouges, les zones radioactives, les cases grises, les obstacles, et la case verte la cible à atteindre. Le robot est également affiché sur la grille.\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "class Simulation:\n",
    "    \"\"\"Coordonne les interactions entre le robot, son capteur et l'environnement.\"\"\"\n",
    "\n",
    "    def __init__(self, robot, environment, sensor, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialise la simulation.\n",
    "\n",
    "        :param robot: Instance du robot.\n",
    "        :param environment: Instance de l'environnement.\n",
    "        :param sensor: Instance du capteur.\n",
    "        :param verbose: Active le mode verbeux (débogage) si à True.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes, max_steps_per_episode, epsilon, alpha, gamma):\n",
    "        \"\"\"\n",
    "        Entraîne le robot pendant un certain nombre d'épisodes.\n",
    "\n",
    "        :param episodes: Nombre d'épisodes.\n",
    "        :param max_steps_per_episode: Nombre maximum de pas par épisode.\n",
    "        :param epsilon: Taux d'exploration.\n",
    "        :param alpha: Taux d'apprentissage.\n",
    "        :param gamma: Remise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def find_best_path(self):\n",
    "        \"\"\"\n",
    "        Détermine le meilleur chemin appris par le robot.\n",
    "\n",
    "        :return: Liste des positions du chemin.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"Affiche l'environnement et le robot avec matplotlib.\"\"\"\n",
    "        grid = self.environment.display()\n",
    "        grid_array = np.array(grid)\n",
    "        cmap = colors.ListedColormap(['lightcoral', 'lightgray', 'white', 'lightgreen'])\n",
    "        bounds = [-2.5, -1.5, -0.5, 0.5, 1.5]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        ax.imshow(grid_array, cmap=cmap, norm=norm)\n",
    "\n",
    "        for (x, y) in self.environment.obstacles:\n",
    "            ax.text(x, y, 'X', va='center', ha='center', color='black', fontsize=12, fontweight='bold')\n",
    "        for (x, y) in self.environment.radioactive_zones:\n",
    "            ax.text(x, y, 'R', va='center', ha='center', color='black', fontsize=12, fontweight='bold')\n",
    "        tx, ty = self.environment.get_target_position()\n",
    "        ax.text(tx, ty, 'T', va='center', ha='center', color='black', fontsize=12, fontweight='bold')\n",
    "\n",
    "        rx, ry = self.robot.position\n",
    "        ax.plot(rx, ry, 'o', color='dodgerblue', markersize=12, label='Robot')\n",
    "\n",
    "        ax.set_xticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.grid(color='gray', linestyle='-', linewidth=1)\n",
    "        ax.set_xlim(-0.5, self.environment.size - 0.5)\n",
    "        ax.set_ylim(self.environment.size - 0.5, -0.5)\n",
    "\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], marker='s', color='w', label='Vide', markerfacecolor='white', markersize=15),\n",
    "            Line2D([0], [0], marker='s', color='w', label='Obstacle', markerfacecolor='lightgray', markersize=15),\n",
    "            Line2D([0], [0], marker='s', color='w', label='Radioactif', markerfacecolor='lightcoral', markersize=15),\n",
    "            Line2D([0], [0], marker='s', color='w', label='Cible', markerfacecolor='lightgreen', markersize=15),\n",
    "            Line2D([0], [0], marker='o', color='w', label='Robot', markerfacecolor='dodgerblue', markersize=15),\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "        plt.title('Environnement')\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_best_path(self):\n",
    "        \"\"\"Affiche le meilleur chemin trouvé par le robot.\"\"\"\n",
    "        best_path = self.find_best_path()\n",
    "        grid = self.environment.display()\n",
    "        # Zone radioactive : lightcoral ; obstacle : lightgray ; case vide : white ; cible : lightgreen\n",
    "        cmap = colors.ListedColormap(['lightcoral', 'lightgray', 'white', 'lightgreen'])\n",
    "        bounds = [-2, -1, 0, 1, 2]\n",
    "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(grid, cmap=cmap, norm=norm,\n",
    "                  extent=[-0.5, self.environment.size - 0.5, self.environment.size - 0.5, -0.5])\n",
    "\n",
    "        # Ajoute du texte aux cases avec obstacle ('x'), zone radioactive ('r'), cible ('T') et robot ('Robot')\n",
    "        for obstacle in self.environment.obstacles:\n",
    "            ax.text(obstacle[0], obstacle[1], 'x', va='center', ha='center', color='black')\n",
    "        for radioactive in self.environment.radioactive_zones:\n",
    "            ax.text(radioactive[0], radioactive[1], 'r', va='center', ha='center', color='black')\n",
    "        target = self.environment.get_target_position()\n",
    "        ax.text(target[0], target[1], 'T', va='center', ha='center', color='black')\n",
    "        robot_pos = self.robot.position\n",
    "        ax.text(robot_pos[0], robot_pos[1], 'R', va='center', ha='center', color='black')\n",
    "\n",
    "        # Dessine le meilleur chemin en pointillé et en bleu\n",
    "        for (x, y) in best_path:\n",
    "            ax.plot(x, y, 'bo')\n",
    "\n",
    "        if len(best_path) > 1:\n",
    "            for i in range(len(best_path) - 1):\n",
    "                start = best_path[i]\n",
    "                end = best_path[i + 1]\n",
    "                ax.plot([start[0], end[0]], [start[1], end[1]], 'b--')\n",
    "\n",
    "        # Définit la grille\n",
    "        ax.set_xticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.set_yticks(np.arange(-0.5, self.environment.size, 1))\n",
    "        ax.grid(which='major', color='gray', linestyle='-', linewidth=1)\n",
    "\n",
    "        # Définit les limites des axes\n",
    "        ax.set_xlim(-0.5, self.environment.size - 0.5)\n",
    "        ax.set_ylim(-0.5, self.environment.size - 0.5)\n",
    "\n",
    "        # Enlève les étiquettes des axes\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        # Inverse l'axe vertical pour avoir l'origine en bas à gauche\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        plt.show()\n",
    "        plt.pause(0.001)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "- `train()` entraîne le robot sur plusieurs épisodes ;\n",
    "- `find_best_path()` utilise la table Q apprise pour trouver le meilleur chemin vers l'objectif.\n",
    "\n",
    "#### Créer et entraîner une simulation\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "# Exemple d'utilisation de la classe Simulation\n",
    "env = Environment(size=5, obstacle_count=2, radioactive_count=2, target_position=(4, 4))\n",
    "sensor = Sensor(range_detection=1)\n",
    "robot = Robot(start_position=(0, 0))\n",
    "simulation = Simulation(robot, env, sensor)\n",
    "\n",
    "# Paramètres de la simulation\n",
    "episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "epsilon = 0.1  # Probabilité de choisir une action aléatoire\n",
    "alpha = 0.1    # Taux d'apprentissage\n",
    "gamma = 0.9    # Facteur de remise\n",
    "\n",
    "# Visualiser l'environnement avant entraînement\n",
    "simulation.visualize()\n",
    "\n",
    "# Entraîner le robot\n",
    "simulation.train(episodes, max_steps_per_episode, epsilon, alpha, gamma)\n",
    "\n",
    "# Visualiser l'environnement après entraînement\n",
    "simulation.visualize()\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Trouver et visualiser le meilleur chemin\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: dashed;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\">\n",
    "\n",
    "```python\n",
    "simulation.visualize_best_path()\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0261f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dfae5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Cette séance vous a permis de mettre en oeuvre l'algorithme Q-learning dans des environnements simples, puis de l'appliquer à un cas plus complexe impliquant la navigation d'un robot dans une grille avec obstacles et zones dangereuses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
