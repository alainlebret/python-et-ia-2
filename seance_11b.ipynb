{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c1e5db-b21e-4588-b942-3c76634e63b1",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: solid;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\n",
    "  border-radius: 10px;\">\n",
    "  \n",
    "# **Python et intelligence artificielle**\n",
    "\n",
    "# *Complément au mécanisme de la rétropropagation*\n",
    "\n",
    "</div>\n",
    "\n",
    "Dans ce complément aux bloc-notes de la séance n°11, nous souhaitons vous amener à mieux comprendre le mécanisme de rétropropagation en l'appliquant à un unique neurone. Nous allons réaliser quelques étapes en utilisant les concepts théoriques de base, comme la dérivée d'une fonction et la règle de dérivation en chaîne, afin de valider les calculs des gradients et vérifier la modification des poids. \n",
    "\n",
    "Rappelons tout d'abord l'implémentation de la classe `Neurone` :\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: dashed;\n",
    "    border-width: 1px;\n",
    "    border-color: gray;\">\n",
    "\n",
    "```python\n",
    "class Neurone:\n",
    "    def __init__(self, nbr_entrees, fonction_activation='sigmoïde'):\n",
    "        # Initialisation des poids et du biais\n",
    "        self.poids = np.random.uniform(-0.1, 0.1, nbr_entrees)\n",
    "        self.biais = np.random.normal(0, 1e-3)\n",
    "        # Définition de la fonction d'activation\n",
    "        self.fonction_activation = fonction_activation\n",
    "        \n",
    "    def _appliquer_activation(self, x):\n",
    "        \"\"\"Applique la fonction d'activation sélectionnée.\"\"\"\n",
    "        if self.fonction_activation == 'sigmoïde':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.fonction_activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.fonction_activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Fonction d'activation non reconnue.\")\n",
    "\n",
    "    def calculer_sortie(self, entrees):\n",
    "        \"\"\" Calcule la sortie du neurone \"\"\"\n",
    "        somme = np.dot(entrees, self.poids) + self.biais\n",
    "        return self._appliquer_activation(somme)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "### 1. **Dérivée de la fonction d'activation**\n",
    "\n",
    "Le calcul de la dérivée de la fonction d'activation est utilisée pour la rétropropagation car elle sert à déterminer comment ajuster les poids afin de réduire l'erreur du modèle. La dérivée nous indique comment la sortie change pour une variation infime de l'entrée, ce qui va nous renseigner sur la manière dont nous devons ajuster chaque poids durant l'apprentissage.\n",
    "\n",
    "Prenons la fonction **sigmoïde** :\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Cette fonction est couramment utilisée pour les neurones de sortie car elle produit des valeurs entre 0 et 1 ce qui est adapté à des problèmes de classification. Sa dérivée qui mesure la \"pente\" ou la \"sensibilité\" de cette fonction pour une valeur donnée de $z$, est définie par :\n",
    "\n",
    "$$\n",
    "\\phi'(z) = \\phi(z) \\cdot (1 - \\phi(z))\n",
    "$$\n",
    "\n",
    "Vérifions que cette version analytique de la dérivée est correcte en utilisant le **quotient de différence symétrique**. Ce quotient permet d'approcher la dérivée d'une fonction en observant les changements de celle-ci pour une petite variation de son argument. Le quotient qui permet d'approximer $\\phi'(z)$ est calculé de la manière suivante :\n",
    "\n",
    "$$\n",
    "\\phi'(z) \\approx \\frac{\\phi(z + h) - \\phi(z - h)}{2h}\n",
    "$$\n",
    "\n",
    "où $h$ est un nombre très petit (par exemple $10^{-5}$). Cette formule revient à calculer la pente de la droite reliant les points $(z + h, \\phi(z + h))$ et $(z - h, \\phi(z - h))$. Cette \"pente\" approche la dérivée si $h$ est suffisamment petit.\n",
    "\n",
    "Cette vérification sert uniquement à confirmer que la dérivée analytique $\\phi'(z) = \\phi(z) \\cdot (1 - \\phi(z))$ est correcte. Les valeurs devraient être très proches si notre implémentation est correcte. Vérifiez-le pour la valeur $z = 0.5$.\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bef750",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dc6fa",
   "metadata": {},
   "source": [
    "\n",
    "#### Sortie\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: solid;\n",
    "    border-width: 1px;\n",
    "    border-color: lightgray;\">\n",
    "\n",
    "```python\n",
    "Dérivée analytique : 0.2350037122015945\n",
    "Dérivée numérique : 0.2350037122067494\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "## 2. **Calcul du gradient pour une seule sortie**\n",
    "\n",
    "Pour un neurone unique, le gradient de l'erreur par rapport aux poids peut être calculé en appliquant\n",
    "la [règle de dérivation en chaîne](https://fr.wikipedia.org/wiki/Théorème_de_dérivation_des_fonctions_composées). \n",
    "Ce gradient va nous indiquer dans quelle mesure chaque poids contribue à l'erreur totale. \n",
    "\n",
    "Rappelons que pour un neurone produisant une sortie $y$ avec une cible $y_{\\text{cible}}$, l'erreur quadratique est donnée par la fonction de coût :\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}(y_{\\text{cible}} - y)^2\n",
    "$$\n",
    "\n",
    "Pour ajuster les poids, nous devons savoir comment $J$ varie par rapport à chaque poids $w_i$, c'est-à-dire que nous devons calculer $\\frac{\\partial J}{\\partial w_i}$ en appliquant la règle de dérivation en chaîne. Cela se fait en deux étapes :\n",
    "\n",
    "1. Calculons $\\delta$, l'écart entre la sortie $y$ et la cible $y_{\\text{cible}}$ :\n",
    "\n",
    "   $$\n",
    "   \\delta = y - y_{\\text{cible}}\n",
    "   $$\n",
    "\n",
    "   Cela nous indique de combien le neurone s'éloigne de la cible.\n",
    "\n",
    "2. En utilisant la règle de dérivation en chaîne, nous obtenons pour le gradient de $J$ en fonction des poids :\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial w_i} = \\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "   $$\n",
    "\n",
    "   avec :\n",
    "\n",
    "   - $\\delta$ mesure l'erreur de sortie ;\n",
    "   - $\\phi'(z)$ représente la sensibilité de la fonction d'activation aux changements de $z$ ;\n",
    "   - $x_i$ est l'entrée associée au poids $w_i$.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "Prenons un neurone avec deux entrées initialisées respectivement à $0,6$ et $0,1$, et une sortie cible fixée à $0,8$. En comparant le gradient analytique et celui produit par votre classe, vérifiez la cohérence des calculs à $10^{-5}$ près.\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0ecc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23195f",
   "metadata": {},
   "source": [
    "\n",
    "#### Sortie\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: solid;\n",
    "    border-width: 1px;\n",
    "    border-color: lightgray;\">\n",
    "\n",
    "```python\n",
    "Gradient analytique : [-0.05987721 -0.00997953]\n",
    "Gradient pour la classe : [-0.05987721 -0.00997953]\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "## 3. **Rétropropagation avec des valeurs simples**\n",
    "\n",
    "La rétropropagation calcule les ajustements nécessaires des poids en utilisant le gradient de la fonction de coût par rapport à chaque poids. Cela permet de minimiser l'erreur de prédiction du neurone. Pour y parvenir, il est nécessaire de suivre une série d'étapes :\n",
    "\n",
    "L'algorithme de rétropropagation pour un neurone unique peut se résumer aux étapes suivantes :\n",
    "\n",
    "1. **Propagation avant**\n",
    "   \n",
    "   Calculer $z$ et $y$ en utilisant les poids actuels.\n",
    "2. **Calcul de l'erreur**\n",
    "\n",
    "   Déterminer $\\delta = y - y_{\\text{cible}}$.\n",
    "3. **Calcul des gradients**\n",
    "   \n",
    "   Pour chaque poids $w_i$, calculer :\n",
    "   \n",
    "   $$ \\frac{\\partial J}{\\partial w_i} = \\delta \\cdot \\phi'(z) \\cdot x_i $$\n",
    "4. **Mise à jour des poids**\n",
    "   \n",
    "   Ajuster chaque poids en utilisant la descente de gradient :\n",
    "   \n",
    "   $$ w_i = w_i - \\alpha \\cdot \\frac{\\partial J}{\\partial w_i} $$\n",
    "\n",
    "Ces étapes permettent au neurone d'apprendre à ajuster ses poids de manière à minimiser l'erreur de prédiction au fil des itérations.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "Nous allons utiliser des valeurs simples pour les poids, le biais, les entrées et la sortie cible, de sorte que vous puissiez calculer les résultats attendus manuellement. \n",
    "Considérons un neurone à 2 entrées et 1 sortie dont la fonction d'activation est la fonction sigmoïde. Le neurone est initialisé avec les valeurs suivantes :\n",
    "\n",
    "- Poids : $w_1 = 0.5$, $w_2 = -0.5$, biais $b = 0.0$.\n",
    "- Entrées : $x_1 = 1.0$, $x_2 = 1.0$.\n",
    "- Sortie cible : $y_{\\text{cible}} = 1.0$.\n",
    "- Taux d'apprentissage : $\\alpha = 0.1$.\n",
    "\n",
    "Puis réalisez les étapes de l'algorithme de rétropropagation. Vous répéterez ces étapes sur plusieurs itérations pour observer comment les poids s'ajustent et comment l'erreur diminue progressivement.\n",
    "\n",
    "### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2083b90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05478f9c",
   "metadata": {},
   "source": [
    "\n",
    "#### Sortie\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: solid;\n",
    "    border-width: 1px;\n",
    "    border-color: lightgray;\">\n",
    "\n",
    "```python\n",
    "Mise à jour attendue des poids : [ 0.5125 -0.4875]\n",
    "Poids après rétropropagation : [ 0.5125 -0.4875]\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Détail sur l'obtention de la dérivée partielle de la fonction de coût par rapport au poids\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  background-color: lightgray;\n",
    "  border-style: solid;\n",
    "  border-width: 2px;\n",
    "  border-color: khaki;\">\n",
    "\n",
    "L'expression\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = \\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "$$\n",
    "\n",
    "provient de l'application de la règle de dérivation en chaîne qui permet d'obtenir la dérivée de la fonction de coût $J$ par rapport au poids $w_i$ d'un neurone.\n",
    "\n",
    "Considérons que le neurone produit une sortie $y$, calculée en appliquant une fonction d'activation $\\phi$ à une combinaison linéaire des entrées pondérées. La sortie cible est $y_{\\text{cible}}$, et la fonction de coût est ici l'erreur quadratique moyenne :\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2} (y_{\\text{cible}} - y)^2\n",
    "$$\n",
    "\n",
    "où $y = \\phi(z)$ et $z$ est la somme pondérée des entrées, soit :\n",
    "\n",
    "$$\n",
    "z = \\sum_{i} w_i x_i + b\n",
    "$$\n",
    "\n",
    "Nous voulons connaître l'impact d'un changement du poids $w_i$ sur la fonction de coût $J$, c'est-à-dire calculer la dérivée partielle $\\frac{\\partial J}{\\partial w_i}$. Pour ce faire, nous appliquons la règle de dérivation en chaîne, en décomposant $\\frac{\\partial J}{\\partial w_i}$ en plusieurs étapes.\n",
    "\n",
    "Tout d'abord, calculons la dérivée de $J$ par rapport à la sortie $y$ :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\frac{1}{2} (y_{\\text{cible}} - y)^2 \\right) = -(y_{\\text{cible}} - y) = -\\delta\n",
    "$$\n",
    "\n",
    "où $\\delta = y - y_{\\text{cible}}$ est l'erreur de prédiction du neurone.\n",
    "\n",
    "Puisque $y = \\phi(z)$, la dérivée de $y$ par rapport à $z$ est la dérivée de la fonction d'activation :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial z} = \\phi'(z)\n",
    "$$\n",
    "\n",
    "Ainsi, pour propager l'erreur $\\delta$ vers $z$, nous devons multiplier par $\\phi'(z)$.\n",
    "\n",
    "Enfin, $z$ dépend de chaque poids $w_i$ et de l'entrée $x_i$ selon la relation $z = \\sum_{i} w_i x_i + b$. La dérivée de $z$ par rapport à $w_i$ est simplement :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w_i} = x_i\n",
    "$$\n",
    "\n",
    "En appliquant la règle de dérivation en chaîne, nous obtenons :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "Substituons les dérivées obtenues par leur valeur :\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial y} = -\\delta$,\n",
    "- $\\frac{\\partial y}{\\partial z} = \\phi'(z)$,\n",
    "- $\\frac{\\partial z}{\\partial w_i} = x_i$.\n",
    "\n",
    "Ce qui nous donne au final :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = -\\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "$$\n",
    "\n",
    "Puisque $-\\delta$ est simplement l'opposée de l'erreur $\\delta$, on écrit finalement :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = \\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "[Séance n°11a](seance_11a.ipynb) / [Séance n°11c](seance_11c.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
