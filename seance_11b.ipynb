{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c1e5db-b21e-4588-b942-3c76634e63b1",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  border-style: solid;\n",
    "  border-width: 1px;\n",
    "  border-color: gray;\n",
    "  border-radius: 10px;\">\n",
    "  \n",
    "# **Python et intelligence artificielle**\n",
    "\n",
    "# *Complément au mécanisme de la rétropropagation*\n",
    "\n",
    "</div>\n",
    "\n",
    "Dans ce complément aux bloc-notes de la séance n°11, nous souhaitons vous amener à mieux comprendre le mécanisme de rétropropagation en l'appliquant à un unique neurone. Nous allons réaliser quelques étapes en utilisant les concepts théoriques de base, comme la dérivée d'une fonction et la règle de dérivation en chaîne, afin de valider les calculs des gradients et vérifier la modification des poids. \n",
    "\n",
    "Rappelons tout d'abord l'implémentation de la classe `Neurone` :\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: dashed;\n",
    "    border-width: 1px;\n",
    "    border-color: gray;\">\n",
    "\n",
    "```python\n",
    "class Neurone:\n",
    "    def __init__(self, nbr_entrees, fonction_activation='sigmoïde'):\n",
    "        # Initialisation des poids et du biais\n",
    "        self.poids = np.random.uniform(-0.1, 0.1, nbr_entrees)\n",
    "        self.biais = np.random.normal(0, 1e-3)\n",
    "        # Définition de la fonction d'activation\n",
    "        self.fonction_activation = fonction_activation\n",
    "        \n",
    "    def _appliquer_activation(self, x):\n",
    "        \"\"\"Applique la fonction d'activation sélectionnée.\"\"\"\n",
    "        if self.fonction_activation == 'sigmoïde':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.fonction_activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.fonction_activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Fonction d'activation non reconnue.\")\n",
    "\n",
    "    def calculer_sortie(self, entrees):\n",
    "        \"\"\" Calcule la sortie du neurone \"\"\"\n",
    "        somme = np.dot(entrees, self.poids) + self.biais\n",
    "        return self._appliquer_activation(somme)\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "### 1. **Dérivée de la fonction d'activation**\n",
    "\n",
    "Le calcul de la dérivée de la fonction d'activation est utilisée pour la rétropropagation car elle sert à déterminer comment ajuster les poids afin de réduire l'erreur du modèle. La dérivée nous indique comment la sortie change pour une variation infime de l'entrée, ce qui est nécessaire pour savoir comment ajuster chaque poids durant l'apprentissage.\n",
    "\n",
    "Prenons la fonction **sigmoïde** :\n",
    "\n",
    "$$\n",
    "\\phi(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Cette fonction est couramment utilisée pour les neurones de sortie car elle produit des valeurs entre 0 et 1 ce qui est adapté à des problèmes de classification. Sa dérivée qui mesure la \"pente\" ou la \"sensibilité\" de cette fonction pour une valeur donnée de $z$, est féfinie par :\n",
    "\n",
    "$$\n",
    "\\phi'(z) = \\phi(z) \\cdot (1 - \\phi(z))\n",
    "$$\n",
    "\n",
    "Vérifions que notre dérivée analytique de la sigmoïde est correcte en utilisant le **quotient de différence symétrique** qui permet d'approcher la dérivée d'une fonction en observant les changements de cette fonction pour une petite variation de son argument. Ce quotient pour approximer $\\phi'(z)$ est calculé de la manière suivante :\n",
    "\n",
    "$$\n",
    "\\phi'(z) \\approx \\frac{\\phi(z + h) - \\phi(z - h)}{2h}\n",
    "$$\n",
    "\n",
    "où $h$ est un très petit nombre (par exemple, $10^{-5}$). Cette formule revient à calculer la pente de la ligne reliant les points $(z + h, \\phi(z + h))$ et $(z - h, \\phi(z - h))$. Cette \"pente\" approche la dérivée si $h$ est suffisamment petit.\n",
    "\n",
    "Cette vérification sert uniquement à confirmer que la dérivée analytique $\\phi'(z) = \\phi(z) \\cdot (1 - \\phi(z))$ est correcte. Les valeurs devraient être très proches si notre implémentation est correcte. Vérifiez-le pour la valeur $z = 0.5$.\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bef750",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2dc6fa",
   "metadata": {},
   "source": [
    "\n",
    "#### Sortie\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: solid;\n",
    "    border-width: 1px;\n",
    "    border-color: lightgray;\">\n",
    "\n",
    "```python\n",
    "Dérivée analytique : 0.2350037122015945\n",
    "Dérivée numérique : 0.2350037122067494\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "## 2. **Calcul du gradient pour une seule sortie**\n",
    "\n",
    "Pour un neurone unique, le gradient de l'erreur par rapport aux poids peut être calculé en appliquant\n",
    "la [règle de dérivation en chaîne](https://fr.wikipedia.org/wiki/Théorème_de_dérivation_des_fonctions_composées). \n",
    "Ce gradient va nous indiquer dans quelle mesure chaque poids contribue à l'erreur totale. \n",
    "\n",
    "Rappelons que pour un neurone produisant une sortie $y$ avec une cible $y_{\\text{cible}}$, l'erreur quadratique est donnée par la fonction de coût :\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2}(y_{\\text{cible}} - y)^2\n",
    "$$\n",
    "\n",
    "Pour ajuster les poids, nous devons savoir comment $J$ varie par rapport à chaque poids $w_i$, c'est-à-dire que nous devons calculer $\\frac{\\partial J}{\\partial w_i}$ en appliquant la règle de dérivation en chaîne. Cela se fait en deux étapes :\n",
    "\n",
    "1. Calculons $\\delta$, l'écart entre la sortie $y$ et la cible $y_{\\text{cible}}$ :\n",
    "\n",
    "   $$\n",
    "   \\delta = y - y_{\\text{cible}}\n",
    "   $$\n",
    "\n",
    "   Cela nous indique de combien le neurone s'éloigne de la cible.\n",
    "\n",
    "2. En utilisant la règle de dérivation en chaîne, nous obtenons pour le gradient de $J$ en fonction des poids :\n",
    "\n",
    "   $$\n",
    "   \\frac{\\partial J}{\\partial w_i} = \\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "   $$\n",
    "\n",
    "   avec :\n",
    "\n",
    "   - $\\delta$ mesure l'erreur de sortie ;\n",
    "   - $\\phi'(z)$ représente la sensibilité de la fonction d'activation aux changements de $z$ ;\n",
    "   - $x_i$ est l'entrée associée au poids $w_i$.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "Prenons un neurone avec deux entrées initialisées respectivement à $0,6$ et $0,1$, et une sortie cible fixée à $0,8$. En comparant le gradient analytique et celui produit par votre classe, vérifiez la cohérence des calculs à $10^{-5}$ près.\n",
    "\n",
    "#### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0ecc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23195f",
   "metadata": {},
   "source": [
    "\n",
    "#### Sortie\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: solid;\n",
    "    border-width: 1px;\n",
    "    border-color: lightgray;\">\n",
    "\n",
    "```python\n",
    "Gradient analytique : [-0.05987721 -0.00997953]\n",
    "Gradient pour la classe : [-0.05987721 -0.00997953]\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "## 3. **Rétropropagation avec des valeurs simples**\n",
    "\n",
    "Nous allons utiliser des valeurs simples pour les poids, les biais, les entrées, et la sortie cible, de sorte que vous puissiez calculer les résultats attendus manuellement.\n",
    "\n",
    "### Exemple\n",
    "\n",
    "1. Pour un neurone à deux entrées, fixez les poids à $0.5$ et $-0.5$ pour chacune des entrées et mettez le biais à $0.0$.\n",
    "2. Initialisez les valeurs des deux entrées à $1.0$, la sortie cible à $1.0$ et le taux d'apprentissage à $0.1$.\n",
    "3. Effectuez la rétropropagation et vérifiez que les valeurs mises à jour des poids et biais correspondent aux résultats que vous aurez calculés manuellement.\n",
    "\n",
    "### Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2083b90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05478f9c",
   "metadata": {},
   "source": [
    "\n",
    "#### Sortie\n",
    "\n",
    "<div style=\"\n",
    "    padding: 5pt;\n",
    "    border-style: solid;\n",
    "    border-width: 1px;\n",
    "    border-color: lightgray;\">\n",
    "\n",
    "```python\n",
    "Mise à jour attendue des poids : [ 0.5125 -0.4875]\n",
    "Poids après rétropropagation : [ 0.5125 -0.4875]\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Détail sur l'obtention de la dérivée partielle de la fonction de coût par rapport au poids\n",
    "\n",
    "<div style=\"\n",
    "  padding: 5pt;\n",
    "  background-color: lightgray;\n",
    "  border-style: solid;\n",
    "  border-width: 2px;\n",
    "  border-color: khaki;\">\n",
    "\n",
    "L'expression\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = \\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "$$\n",
    "\n",
    "provient de l'application de la règle de dérivation en chaîne qui permet d'obtenir la dérivée de la fonction de coût $J$ par rapport au poids $w_i$ d'un neurone.\n",
    "\n",
    "Considérons que le neurone produit une sortie $y$, calculée en appliquant une fonction d'activation $\\phi$ à une combinaison linéaire des entrées pondérées. La sortie cible est $y_{\\text{cible}}$, et la fonction de coût est ici l'erreur quadratique moyenne :\n",
    "\n",
    "$$\n",
    "J = \\frac{1}{2} (y_{\\text{cible}} - y)^2\n",
    "$$\n",
    "\n",
    "où $y = \\phi(z)$ et $z$ est la somme pondérée des entrées, soit :\n",
    "\n",
    "$$\n",
    "z = \\sum_{i} w_i x_i + b\n",
    "$$\n",
    "\n",
    "Nous voulons connaître l'impact d'un changement du poids $w_i$ sur la fonction de coût $J$, c'est-à-dire calculer la dérivée partielle $\\frac{\\partial J}{\\partial w_i}$. Pour ce faire, nous appliquons la règle de dérivation en chaîne, en décomposant $\\frac{\\partial J}{\\partial w_i}$ en plusieurs étapes.\n",
    "\n",
    "Tout d'abord, calculons la dérivée de $J$ par rapport à la sortie $y$ :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\frac{1}{2} (y_{\\text{cible}} - y)^2 \\right) = -(y_{\\text{cible}} - y) = -\\delta\n",
    "$$\n",
    "\n",
    "où $\\delta = y - y_{\\text{cible}}$ est l'erreur de prédiction du neurone.\n",
    "\n",
    "Puisque $y = \\phi(z)$, la dérivée de $y$ par rapport à $z$ est la dérivée de la fonction d'activation :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial z} = \\phi'(z)\n",
    "$$\n",
    "\n",
    "Ainsi, pour propager l'erreur $\\delta$ vers $z$, nous devons multiplier par $\\phi'(z)$.\n",
    "\n",
    "Enfin, $z$ dépend de chaque poids $w_i$ et de l'entrée $x_i$ selon la relation $z = \\sum_{i} w_i x_i + b$. La dérivée de $z$ par rapport à $w_i$ est simplement :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w_i} = x_i\n",
    "$$\n",
    "\n",
    "En appliquant la règle de dérivation en chaîne, nous obtenons :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = \\frac{\\partial J}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "Substituons les dérivées obtenues par leur valeur :\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial y} = -\\delta$,\n",
    "- $\\frac{\\partial y}{\\partial z} = \\phi'(z)$,\n",
    "- $\\frac{\\partial z}{\\partial w_i} = x_i$.\n",
    "\n",
    "Ce qui nous donne au final :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = -\\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "$$\n",
    "\n",
    "Puisque $-\\delta$ est simplement l'opposée de l'erreur $\\delta$, on écrit finalement :\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w_i} = \\delta \\cdot \\phi'(z) \\cdot x_i\n",
    "$$\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "[Séance n°11a](seance_11a.ipynb) / [Séance n°11c](seance_11c.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
